---
title: "Evaluation Report Template"
description: "Standard format for program evaluation findings"
category: "programs"
subcategory: "Impact Measurement"
tags: ["evaluation", "reporting", "assessment"]
role: ["Program Manager"]
difficulty: "Advanced"
updated: 2025-11-01
---

## The Prompt

```
Create a comprehensive evaluation report template for [ORGANIZATION NAME]’s [PROGRAM NAME] covering [REPORT PERIOD] in [GEOGRAPHY].

Quick start (minimal inputs): If you only provide [ORGANIZATION NAME], [PROGRAM NAME], [REPORT PERIOD], [GEOGRAPHY], and [TONE: BALANCED (default), FORMAL, or WARM], generate the full template using defaults for all other fields.

If key details are missing, first ask these 5 questions, then proceed:
1) Primary audience and uses? [e.g., program staff to improve delivery; leadership to inform planning]
2) Funders/board version needed? [Yes/No; which funder/board?]
3) Key demographics for equity breakdowns? [e.g., age, race/ethnicity, income, language, zip code]
4) Main data sources available? [e.g., surveys, interviews, admin data, observations]
5) Team capacity? [TEAM SIZE/RESOURCES: e.g., 3 staff, part-time evaluator, $50K program budget]

DELIVERABLE: Produce a complete, fill-in-ready template document that includes:
- Section headings in the specified order with per-section word ranges
- Writing prompts in [BRACKETS] for users to complete
- At least one short, concrete example in the Executive Summary and one in the Findings section showing the expected level of specificity
- Sample data table shells and visual placeholders with caption prompts
- “In plain language” callouts translating evaluator terms
- “How to adapt for funders/board” notes within each section
- Equity and participation prompts embedded in methods and findings
- Constructive subheads for mixed/negative results: “What we learned / Why this matters / What we’re changing”

LENGTH TARGET: 10–25 pages total (adjustable). If user does not specify a page target, default to 10–25 pages.

TONE: [BALANCED (default) | FORMAL | WARM]
- BALANCED: Professional and approachable
- FORMAL: Grant/board-facing, neutral
- WARM: Community-centered, accessible

AUDIENCES AND USES:
- Primary audience: [PRIMARY AUDIENCE and intended uses]
- Secondary audience(s): [SECONDARY AUDIENCE and intended uses]
- Funders/Board: [FUNDERS/BOARD if applicable]

TEMPLATE STRUCTURE AND SPECIFICATIONS

1) Executive Summary (350–600 words)
Include prompts for:
- Program purpose and who it serves: [Program aims; target population; high-level needs/context]
- Reach and participation: [# served; eligibility; participation intensity/dosage; key demographics]
- Top 2–3 findings: [Quantitative KPI highlights with %/#; qualitative themes with brief quote]
- High-level recommendations: [Most actionable next steps]
- Overall assessment/learning stance: [Balanced summary; what worked; what to improve]
- In plain language: “This section tells busy readers what changed, for whom, and what we’ll do next.”
- How to adapt for funders/board: Lead with outcomes tied to grant objectives; keep to one page; include grant KPIs and cost-per-outcome if available.

Example (model the level of detail and tone):
“In 2024, the Youth Pathways mentoring program served 127 students in Westview (62% Latinx, 28% Black; 71% eligible for free/reduced lunch). Attendance improved for 89% of participants; average chronic absence dropped from 16% to 9% over two semesters. Students completing 10+ mentoring sessions were 2.1x more likely to submit all homework on time. Youth and caregiver interviews highlighted stronger school belonging and improved communication at home. We recommend formalizing caregiver touchpoints and expanding peer mentoring to all cohorts.”

2) Program Overview (300–450 words)
Prompts:
- Goals and theory of change: [Brief aims; core assumptions]
- Target population and eligibility: [Age/grade; geography; barriers addressed]
- Activities and dosage: [What, how often, by whom; training/credentials]
- Staffing/resources: [TEAM SIZE/RESOURCES; partnerships; budget range]
- Context: [Relevant policy/school-year cycles; community factors]
- Logic model reference: [Appendix reference]
- Sample phrasing: “In [YEAR], [PROGRAM] served [#] participants through [KEY ACTIVITIES] to achieve [OUTCOMES].”
- How to adapt for funders/board: Align goals and activities with funded objectives and approved budget.

3) Evaluation Questions & Methodology (300–500 words)
Prompts:
- Evaluation questions (3–5): [e.g., To what extent did participants improve in [OUTCOME]? Which components were most/least helpful and for whom?]
- Design/approach: [Pre-post, cohort tracking, contribution not attribution; any comparison group]
- Data sources: [DATA SOURCES: surveys, interviews, focus groups, admin records, observations; instrument names]
- Sampling and timing: [Who, how many, when; response rates]
- Data quality checks: [Missing data review, reliability checks, spot audits, data cleaning steps]
- Ethics/consent: [Consent process; privacy; IRB status if applicable]
- Equity/participation: [Whose voices shaped questions? Any community reviewers? Accessibility considerations]
- In plain language: “We measured success by [brief description of indicators and how collected].”
- Table shell: Method | Sample/Response Rate | Timeline | Purpose/Question(s) Informed
- How to adapt for funders/board: Keep design description concise, emphasize validity, consent, and alignment with grant indicators.

4) Findings by Outcome Area (400–700 words per outcome)
For each outcome area, include:
- Outcome definition and indicator(s): [Define; list KPIs and targets]
- Quantitative results:
  - KPI table shell: Indicator | Target | Actual | % Achieved | Data Source | Notes
  - Visual placeholder suggestions:
    - Pre/post change → Paired bar or line chart
    - Achievement rates → Simple bar chart
    - Demographic distribution → Stacked bar chart
  - Caption prompts: “Participants showed [X]% improvement in [OUTCOME] from [BASELINE] to [ENDLINE] (n=[N]).”
- Qualitative insights:
  - Themes: [2–3 themes with brief evidence]
  - Quote placeholder(s): “[Insert 1–2 participant quotes with consent; include ID code]”
- Equity lens:
  - Disaggregate by [KEY DEMOGRAPHICS] and interpret gaps
  - Prompt: “Whose outcomes improved least? Possible drivers? Actionable responses?”
- Mixed/negative results framing:
  - What we learned: [Specific finding]
  - Why this matters: [Implication for clients/operations/equity]
  - What we’re changing: [Concrete adjustment and when]
- Evidence tags: [Cite source: “Survey Q12,” “Admin attendance record,” “Focus Group 2”]
- How to adapt for funders/board: Lead with grant KPIs; include 1–2 concise visuals; footnote methods; keep quotes brief.

Example (model the expected specificity):
Outcome: Improved on-time homework submission
- Indicator: % of students submitting ≥90% of assignments
- Target: 70%; Actual: 64% (n=114); % Achieved: 91%
- Pre/post: 41% → 64% (+23 points)
- Disaggregation: Grades 9–10: 68%; Grades 11–12: 58%; English learners: 61%; Non-EL: 66%
- Theme: Students credited text reminders and peer study halls. Quote: “The Tuesday study group kept me from falling behind.” — 10th grader (consented)
- What we’re changing: Expand peer study halls to 2x/week for upper grades; pilot multilingual reminders.

5) Discussion/Interpretation (300–500 words)
Prompts:
- Connect findings to program theory: [Which components likely drove change, for whom, and why]
- Triangulate: [How quantitative and qualitative converge/diverge]
- External factors: [School calendar, transportation, policy shifts]
- Surprises: [What was unexpected and how we validated it]
- Learning stance:
  - What we learned / Why this matters / What we’re changing
- In plain language: “Here we make sense of why results look the way they do.”
- How to adapt for funders/board: Emphasize accountability, practical lessons, and planned course corrections.

6) Recommendations & Next Steps (250–400 words)
Prompts:
- Tie each recommendation to an insight and an outcome gap
- Make steps specific, time-bound, and feasible given [TEAM SIZE/RESOURCES]
- Table shell:
  - Recommendation | Owner/Lead | Timeframe (immediate/3–6 months/12 months) | Resource Needs (staff hours, budget) | Feasibility (H/M/L) | Success Measure
- Sample recommendations:
  - “Expand peer mentoring to all cohorts | Program Manager | 6 months | 20 staff hours, $2K | High | ≥75% cohort participation”
  - “Standardize caregiver outreach script and schedule | Family Liaison | Immediate | 8 hours setup | High | 80% reach within first month”
- How to adapt for funders/board: Flag any budget-neutral options; identify items requiring reallocation or future grant funding.

7) Limitations (150–250 words)
Prompts:
- Sample size/response rates and representativeness
- Data quality issues: [Missingness, self-report bias]
- Design constraints: [No comparison group; short follow-up window]
- External influences
- Ethics/consent status: [All quotes used with explicit permission; data stored securely]
- In plain language: “Here’s what we couldn’t measure or are less sure about.”
- How to adapt for funders/board: Be transparent and concise; emphasize mitigation steps.

8) Appendices (checklist with placeholders)
- [ ] Logic model or theory of change
- [ ] Data collection instruments (surveys, interview/focus group guides)
- [ ] Detailed data tables (by demographic subgroup)
- [ ] Sample consent forms and data privacy statement
- [ ] Complete indicator list with definitions and calculation notes
- [ ] Additional participant quotes (with consent documentation)
- [ ] Data quality checks documentation (missing data rates; reliability)

Formatting and style requirements
- Define all acronyms on first use
- Use specific numbers and percentages (avoid “many/most”)
- Tie each claim to a data source
- Use simple, plain English; include “In plain language” callouts where technical
- Visuals: include captions, n-sizes, and data sources
- Page target: [PAGE TARGET if specified; otherwise 10–25 pages]

Equity and participation integration
- For each outcome: disaggregate by [KEY DEMOGRAPHICS], interpret gaps, and propose actions
- Prompts: “Whose voices informed this evaluation?” “Who is missing and how will we include them next time?”
- Note access and language accommodations in data collection
- Encourage participatory steps: [participant reviewers; co-interpretation sessions]

Quality standards
Do:
- Use realistic nonprofit examples (e.g., volunteer training completion, client retention, attendance)
- Present successes and challenges honestly
- Make recommendations concrete, resourced, and sequenced
Avoid:
- Hype, defensiveness, or unexplained jargon
- Vague claims or recommendations without owners/timelines

Document details (front matter fields)
- Prepared by: [AUTHOR NAME/TITLE]
- In collaboration with: [EVALUATOR NAME/ORGANIZATION, if external]
- Date: [DATE]
- Report covers: [REPORT PERIOD]
- Geographic scope: [GEOGRAPHY]
- Audiences and uses: [PRIMARY/SECONDARY/FUNDERS-BOARD]

OUTPUT INSTRUCTIONS FOR AI
- Generate a complete, fill-in-ready template document with all sections above, writing prompts in [BRACKETS], table shells, visual placeholders, “In plain language” callouts, equity prompts, and “How to adapt for funders/board” notes within each section.
- Include at least one concise, concrete example paragraph in the Executive Summary and one in the Findings section to demonstrate expected specificity and tone.
- If the user provided only quick-start inputs, apply sensible defaults:
  - Tone: BALANCED
  - Audiences: Primary—program staff/leadership; Secondary—partners/community; Funders/Board—if specified
  - Key demographics: age, race/ethnicity, income, language, geography
  - Data sources: surveys, interviews, admin data
  - Team capacity: “3-person team; part-time evaluator; $50K program budget”
- End the template with a short “How to refine” note for users, e.g.:
  - “Ask: ‘Shorten the methodology to 200 words’”
  - “Ask: ‘Add a visual for the retention KPI and write a caption’”
  - “Ask: ‘Rewrite for a FORMAL tone for a board packet’”
```

## How to Customize

1. Replace all [BRACKETED] fields with your specific information
2. Adjust tone and length as needed for your audience
3. Review and personalize before using

## Pro Tips

1. Test this prompt with your preferred AI tool before using in production
2. Always review AI output for accuracy and appropriateness
3. Customize outputs to match your organization's voice and brand

## Related Prompts

(See other prompts in the programs category)
